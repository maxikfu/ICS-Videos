Topic: Statistical Parsing. The characters in Damon Runyon's short stories are willing to bet "on any proposition whatever", as Runyon says about Sky Masterson in The Idyll of Miss Sarah Brown, from the probability of getting aces back-to-back to the odds against a man being able to throw a peanut from second base to home plate. There is a moral here for language processing: with enough knowledge we can figure the probability of just about anything. The last two chapters have introduced sophisticated models of syntactic structure and its parsing. Here, we show that it is possible to build probabilistic models of syntactic knowledge and use some of this probabilistic knowledge to build efficient probabilistic parsers. One crucial use of probabilistic parsing is to solve the problem of disambiguation. Recall from Chapter 11 that sentences on average tend to be syntactically ambiguous because of phenomena like coordination ambiguity and attachment ambiguity. The CKY parsing algorithm can represent these ambiguities in an efficient way but is not equipped to resolve them. A probabilistic parser offers a solution to the problem: compute the probability of each interpretation and choose the most probable interpretation. Thus, due to the prevalence of ambiguity, most modern parsers used for natural language understanding tasks (semantic analysis, summarization, question-answering, machine translation) are of necessity probabilistic. The most commonly used probabilistic grammar formalism is the probabilistic context-free grammar (PCFG), a probabilistic augmentation of context-free grammars in which each rule is associated with a probability. We introduce PCFGs in the next section, showing how they can be trained on Treebank grammars and how they can be parsed. We present the most basic parsing algorithm for PCFGs, which is the probabilistic version of the CKY algorithm that we saw in Chapter 11. We then show a number of ways that we can improve on this basic probability model (PCFGs trained on Treebank grammars). One method of improving a trained Treebank grammar is to change the names of the non-terminals. By making the non-terminals sometimes more specific and sometimes more general, we can come up with a grammar with a better probability model that leads to improved parsing scores. Another augmentation of the PCFG works by adding more sophisticated conditioning factors, extending PCFGs to handle probabilistic subcategorization information and probabilistic lexical dependencies. Heavily lexicalized grammar formalisms such as Lexical-Functional Grammar (LFG) (Bresnan, 1982), Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994), Tree-Adjoining Grammar (TAG) (Joshi, 1985), and Combinatory Categorial Grammar (CCG) pose additional problems for probabilistic parsers. Section 12.7 introduces the task of supertagging and the use of heuristic search methods based on the A* algorithm in the context of CCG parsing. Finally, we describe the standard techniques and metrics for evaluating parsers and discuss some relevant psychological results on human parsing. Subtopic:Probabilistic Context-Free Grammars. The simplest augmentation of the context-free grammar is the Probabilistic ContextFree Grammar (PCFG), also known as the Stochastic Context-Free Grammar (SCFG), first proposed by Booth (1969). Recall that a context-free grammar G is defined by four parameters (N , , R, S); a probabilistic context-free grammar is also defined by four parameters, with a slight augmentation to each of the rules in R: N a set of non-terminal symbols (or variables) a set of terminal symbols (disjoint from N ) R a set of rules or productions, each of the form A [ p], where A is a non-terminal, is a string of symbols from the infinite set of strings ( N ), and p is a number between 0 and 1 expressing P( |A) S a designated start symbol That is, a PCFG differs from a standard CFG by augmenting each rule in R with a conditional probability: A [ p] (12.1) Here p expresses the probability that the given non-terminal A will be expanded to the sequence. That is, p is the conditional probability of a given expansion given the left-hand-side (LHS) non-terminal A. We can represent this probability as P(A ) or as P(A |A) or as P(RHS|LHS) Thus, if we consider all the possible expansions of a non-terminal, the sum of their probabilities must be 1: P(A ) = 1 Figure 12.1 shows a PCFG: a probabilistic augmentation of the L1 miniature English CFG grammar and lexicon. Note that the probabilities of all of the expansions of each non-terminal sum to 1. Also note that these probabilities were made up for pedagogical purposes. A real grammar has a great many more rules for each non-terminal; hence, the probabilities of any particular rule would tend to be much smaller. A PCFG is said to be consistent if the sum of the probabilities of all sentences in the language equals 1. Certain kinds of recursive rules cause a grammar to be inconsistent by causing infinitely looping derivations for some sentences. For example, a rule S S with probability 1 would lead to lost probability mass due to derivations that never terminate. See Booth and Thompson (1973) for more details on consistent and inconsistent grammars. Subtopic: PCFGs for Disambiguation. A PCFG assigns a probability to each parse tree T (i.e., each derivation) of a sentence S. This attribute is useful in disambiguation. For example, consider the two parses of the sentence "Book the dinner flight" shown in Fig. 12.2. The sensible parse on the left means "Book a flight that serves dinner". The nonsensical parse on the right, however, would have to mean something like "Book a flight on behalf of `the dinner"' just as a structurally similar sentence like "Can you book John a flight?" means something like "Can you book a flight on behalf of John?" The probability of a particular parse T is defined as the product of the probabilities of all the n rules used to expand each of the n non-terminal nodes in the parse tree T, where each rule i can be expressed as LHSi RHSi. Subtopic: PCFGs for Language Modeling. A second attribute of a PCFG is that it assigns a probability to the string of words constituting a sentence. This is important in language modeling, whether for use in speech recognition, machine translation, spelling correction, augmentative communication, or other applications. The probability of an unambiguous sentence is P(T , S) = P(T ) or just the probability of the single parse tree for that sentence. The probability of an ambiguous sentence is the sum of the probabilities of all the parse trees for the sentence. But the fact that the N -gram model can only make use of a couple words of context means it is ignoring potentially useful prediction cues. Consider predicting the word after in the following sentence from Chelba and Jelinek (2000): (12.13) the contract ended with a loss of 7 cents after trading as low as 9 cents A trigram grammar must predict after from the words 7 cents, while it seems clear that the verb ended and the subject contract would be useful predictors that a PCFGbased parser could help us make use of. Indeed, it turns out that PCFGs allow us to condition on the entire previous context w1 , w2 , ..., wi-1 shown in Eq. 12.11. In summary, this section and the previous one have shown that PCFGs can be applied both to disambiguation in syntactic parsing and to word prediction in language modeling. Both of these applications require that we be able to compute the probability of parse tree T for a given sentence S. The next few sections introduce some algorithms for computing this probability. Subtopic:Probabilistic CKY Parsing of PCFGs. The algorithms for computing the most likely parse are simple extensions of the standard algorithms for parsing; most modern probabilistic parsers are based on the probabilistic CKY algorithm, first described by Ney (1991). As with the CKY algorithm, we assume for the probabilistic CKY algorithm that the PCFG is in Chomsky normal form. Recall from page ?? that grammars in CNF are restricted to rules of the form A B C, or A w. That is, the right-hand side of each rule must expand to either two non-terminals or to a single terminal. For the CKY algorithm, we represented each sentence as having indices between the words. Subtopic:Ways to Learn PCFG Rule Probabilities. Where do PCFG rule probabilities come from? There are two ways to learn probabilities for the rules of a grammar. The simplest way is to use a treebank, a corpus of already parsed sentences. If we don't have a treebank but we do have a (non-probabilistic) parser, we can generate the counts we need for computing PCFG rule probabilities by first parsing a corpus of sentences with the parser. If sentences were unambiguous, it would be as simple as this: parse the corpus, increment a counter for every rule in the parse, and then normalize to get probabilities. But wait! Since most sentences are ambiguous, that is, have multiple parses, we don't know which parse to count the rules in. Instead, we need to keep a separate count for each parse of a sentence and weight each of these partial counts by the probability of the parse it appears in. Subtopic:Problems with PCFGs. While probabilistic context-free grammars are a natural extension to context-free grammars, they have two main problems as probability estimators: Poor independence assumptions: CFG rules impose an independence assumption on probabilities, resulting in poor modeling of structural dependencies across the parse tree. Lack of lexical conditioning: CFG rules don't model syntactic facts about specific words, leading to problems with subcategorization ambiguities, preposition attachment, and coordinate structure ambiguities. Because of these problems, most current probabilistic parsing models use some augmented version of PCFGs, or modify the Treebank-based grammar in some way. In the next few sections after discussing the problems in more detail we introduce some of these augmentations. Subtopic:Lack of Sensitivity to Lexical Dependencies. A second class of problems with PCFGs is their lack of sensitivity to the words in the parse tree. Words do play a role in PCFGs since the parse probability includes the probability of a word given a part-of-speech (i.e., from rules like V sleep, NN book, etc.). But it turns out that lexical information is useful in other places in the grammar, such as in resolving prepositional phrase (PP) attachment ambiguities. Since prepositional phrases in English can modify a noun phrase or a verb phrase, when a parser finds a prepositional phrase, it must decide where to attach it into the tree. Consider the following example: (12.19) Workers dumped sacks into a bin. Figure 12.5 shows two possible parse trees for this sentence; the one on the left is the correct parse; Fig. 12.6 shows another perspective on the preposition attachment problem, demonstrating that resolving the ambiguity in Fig. 12.5 is equivalent to deciding whether to attach the prepositional phrase into the rest of the tree at the NP or VP nodes; we say that the correct parse requires VP attachment, and the incorrect parse implies NP attachment. Distribution of subjects from 31,021 declarative sentences; distribution of objects from 7,489 sentences. This tendency is caused by the use of subject position to realize the topic or old information in a sentence (Giv� on, 1990). Pronouns are a way to talk about old information, while non-pronominal ("lexical") noun-phrases are often used to introduce new referents. We talk more about new and old information in Chapter 21. What information in the input sentence lets us know that (12.20) requires NP attachment while (12.19) requires VP attachment? It should be clear that these preferences come from the identities of the verbs, nouns, and prepositions. It seems that the affinity between the verb dumped and the preposition into is greater than the affinity between the noun sacks and the preposition into, thus leading to VP attachment. On the other hand, in (12.20) the affinity between tons and of is greater than that between caught and of, leading to NP attachment. Thus, to get the correct parse for these kinds of examples, we need a model that somehow augments the PCFG probabilities to deal with these lexical dependency statistics for different verbs and prepositions. Coordination ambiguities are another case in which lexical dependencies are the key to choosing the proper parse. Figure 12.7 shows an example from Collins (1999) with two parses for the phrase dogs in houses and cats. Because dogs is semantically a better conjunct for cats than houses (and because most dogs can't fit inside cats), the parse [dogs in [NP houses and cats]] is intuitively unnatural and should be dispreferred. The two parses in Fig. 12.7, however, have exactly the same PCFG rules, and thus a PCFG will assign them the same probability. NP NP NP Noun Prep dogs in PP NP Noun houses Conj and NP Noun cats NP Noun Prep dogs in NP Noun houses NP PP NP Conj and NP Noun cats Subtopic:Improving PCFGs by Splitting Non-Terminals. Let's start with the first of the two problems with PCFGs mentioned above: their inability to model structural dependencies, like the fact that NPs in subject position tend to be pronouns, whereas NPs in object position tend to have full lexical (nonpronominal) form. How could we augment a PCFG to correctly model this fact? One idea would be to split the NP non-terminal into two versions: one for subjects, one for objects. Having two nodes (e.g., NPsubject and NPobject ) would allow us to correctly model their different distributional properties, since we would have different probabilities for the rule NPsubject PRP and the rule NPobject PRP. In addition to splitting these phrasal nodes, we can also improve a PCFG by splitting the pre-terminal part-of-speech nodes (Klein and Manning, 2003b). For example, different kinds of adverbs (RB) tend to occur in different syntactic positions: the most common adverbs with ADVP parents are also and now, with VP parents n't and not, and with NP parents only and just. Thus, adding tags like RB^ADVP, RB^VP, and RB^NP can be useful in improving PCFG modeling. Similarly, the Penn Treebank tag IN can mark a wide variety of parts-of-speech, including subordinating conjunctions (while, as, if), complementizers (that, for), and prepositions (of, in, from). Some of these differences can be captured by parent annotation (subordinating conjunctions occur under S, prepositions under PP), while others require specifically splitting the pre-terminal nodes. Figure 12.9 shows an example from Klein and Manning (2003b) in which even a parent-annotated grammar incorrectly parses works as a noun in to see if advertising works. Splitting preterminals to allow if to prefer a sentential complement results in the correct verbal parse. To deal with cases in which parent annotation is insufficient, we can also handwrite rules that specify a particular node split based on other features of the tree. For example, to distinguish between complementizer IN and subordinating conjunction IN, both of which can have the same parent, we could write rules conditioned on other aspects of the tree such as the lexical identity (the lexeme that is likely to be a complementizer, as a subordinating conjunction). Node-splitting is not without problems; it increases the size of the grammar and hence reduces the amount of training data available for each grammar rule, leading to overfitting. Thus, it is important to split to just the correct level of granularity for a particular training set. Subtopic:Probabilistic Lexicalized CFGs. The previous section showed that a simple probabilistic CKY algorithm for parsing raw PCFGs can achieve extremely high parsing accuracy if the grammar rule symbols are redesigned by automatic splits and merges. In this section, we discuss an alternative family of models in which instead of modifying the grammar rules, we modify the probabilistic model of the parser to allow for lexicalized rules. The resulting family of lexicalized parsers includes the well-known Collins parser (Collins, 1999) and Charniak parser (Charniak, 1997), both of which are publicly available and widely used throughout natural language processing. We show a lexicalized parse tree with head tags in Fig. 12.10, extended from Fig. ??. TOP S(dumped,VBD) NP(workers,NNS) NNS(workers,NNS) VBD(dumped,VBD) workers dumped VP(dumped,VBD) NP(sacks,NNS) PP(into,P) NP(bin,NN) DT(a,DT) NN(bin,NN) a Subtopic:The Collins Parser. Modern statistical parsers differ in exactly which independence assumptions they make. In this section we describe a simplified version of Collins's worth knowing about; see the summary at the end of the chapter. The first intuition of the Collins parser is to think of the right-hand side of every (internal) CFG rule as consisting of a head non-terminal, together with the nonterminals to the left of the head and the non-terminals to the right of the head. In the abstract, we think about these rules as follows: LHS Ln Ln-1 ... L1 H R1 ... Rn-1 Rn (12.24) Since this is a lexicalized grammar, each of the symbols like L1 or R3 or H or LHS is actually a complex symbol representing the category and its head and head tag, like VP(dumped,VP) or NP(sacks,NNS). Now, instead of computing a single MLE probability for this rule, we are going to break down this rule via a neat generative story, a slight simplification of what is called Collins Model 1. This new generative story is that given the left-hand side, we first generate the head of the rule and then generate the dependents of the head, one by one, from the inside out. Each of these generation steps will have its own probability. We also add a special STOP non-terminal at the left and right edges of the rule; this non-terminal allows the model to know when to stop generating dependents on a given side. We generate dependents on the left side of the head until we've generated STOP on the left side of the head, at which point we move to the right side of the head and start generating dependents there until we generate STOP. Subtopic:Probabilistic CCG Parsing. Lexicalized grammar frameworks such as CCG pose problems for which the phrasebased methods we've been discussing are not particularly well-suited. To quickly review, CCG consists of three major parts: a set of categories, a lexicon that associates words with categories, and a set of rules that govern how categories combine in context. Categories can be either atomic elements, such as S and NP, or functions such as (S\NP)/NP which specifies the transitive verb category. Rules specify how functions, their arguments, and other functions combine. For example, the following rule templates, forward and backward function application, specify the way that functions apply to their arguments. X /Y Y X Y X \Y X The first rule applies a function to its argument on the right, while the second looks to the left for its argument. Subtopic:Ambiguity in CCG. As is always the case in parsing, managing ambiguity is the key to successful CCG parsing. The difficulties with CCG parsing arise from the ambiguity caused by the large number of complex lexical categories combined with the very general nature of the grammatical rules. To see some of the ways that ambiguity arises in a categorial framework, consider the following example. (12.34) United diverted the flight to Reno. Our grasp of the role of the flight in this example depends on whether the prepositional phrase to Reno is taken as a modifier of the flight, as a modifier of the entire verb phrase, or as a potential second argument to the verb divert. In a context-free grammar approach, this ambiguity would manifest itself as a choice among the following rules in the grammar. Nominal Nominal PP VP VP PP VP Verb NP PP In a phrase-structure approach we would simply assign the word to to the category P allowing it to combine with Reno to form a prepositional phrase. The subsequent choice of grammar rules would then dictate the ultimate derivation. In the categorial approach, we can associate to with distinct categories to reflect the ways in which it might interact with other elements in a sentence. The fairly abstract combinatoric rules would then sort out which derivations are possible. Therefore, the source of ambiguity arises not from the grammar but rather from the lexicon. Let's see how this works by considering several possible derivations for this example. To capture the case where the prepositional phrase to Reno modifies the flight, we assign the preposition to the category (NP\NP)/NP, which gives rise to the following derivation. Subtopic:CCG Parsing Frameworks. Since the rules in combinatory grammars are either binary or unary, a bottom-up, tabular approach based on the CKY algorithm should be directly applicable to CCG parsing. Recall from Fig. 12.3 that PCKY employs a table that records the location, category and probability of all valid constituents discovered in the input. Given an appropriate probability model for CCG derivations, the same kind of approach can work for CCG parsing. Unfortunately, the large number of lexical categories available for each word, combined with the promiscuity of CCG's combinatoric rules, leads to an explosion in the number of (mostly useless) constituents added to the parsing table. The key to managing this explosion of zombie constituents is to accurately assess and exploit the most likely lexical categories possible for each word -- a process called supertagging. The following sections describe two approaches to CCG parsing that make use of supertags. Section 12.7.4, presents an approach that structures the parsing process as a heuristic search through the use of the A* algorithm. The following section then briefly describes a more traditional maximum entropy approach that manages the search space complexity through the use of adaptive supertagging -- a process that iteratively considers more and more tags until a parse is found. Subtopic:CCG Parsing using the A* Algorithm. The A* algorithm is a heuristic search method that employs an agenda to find an optimal solution. Search states representing partial solutions are added to an agenda based on a cost function, with the least-cost option being selected for further exploration at each iteration. When a state representing a complete solution is first selected from the agenda, it is guaranteed to be optimal and the search terminates. The A* cost function, f (n), is used to efficiently guide the search to a solution. The f -cost has two components: g(n), the exact cost of the partial solution represented by the state n, and h(n) a heuristic approximation of the cost of a solution that makes use of n. When h(n) satisfies the criteria of not overestimating the actual cost, A* will find an optimal solution. Not surprisingly, the closer the heuristic can get to the actual cost, the more effective A* is at finding a solution without having to explore a significant portion of the solution space. When applied to parsing, search states correspond to edges representing completed constituents. As with the PCKY algorithm, edges specify a constituent's start and end positions, its grammatical category, and its f -cost. Here, the g component represents the current cost of an edge and the h component represents an estimate of the cost to complete a derivation that makes use of that edge. The use of A* for phrase structure parsing originated with (Klein and Manning, 2003a), while the CCG approach presented here is based on (Lewis and Steedman, 2014). Using information from a supertagger, an agenda and a parse table are initialized with states representing all the possible lexical categories for each word in the input, along with their f -costs. The main loop removes the lowest cost edge from the agenda and tests to see if it is a complete derivation. If it reflects a complete derivation it is selected as the best solution and the loop terminates. Otherwise, new states based on the applicable CCG rules are generated, assigned costs, and entered into the agenda to await further processing. The loop continues until a complete derivation is discovered, or the agenda is exhausted, indicating a failed parse. The algorithm is given in Fig. 12.11. Heuristic Functions Before we can define a heuristic function for our A* search, we need to decide how to assess the quality of CCG derivations. For the generic PCFG model, we defined the probability of a tree as the product of the probability of the rules that made up the tree. Given CCG's lexical nature, we'll make the simplifying assumption that the probability of a CCG derivation is just the product of the probability of the supertags assigned to the words in the derivation, ignoring the rules used in the derivation. Subtopic:Evaluating Parsers. The standard techniques for evaluating parsers and grammars are called the PARSEVAL measures; they were proposed by Black et al. (1991) and were based on the same ideas from signal-detection theory that we saw in earlier chapters. The intuition of the PARSEVAL metric is to measure how much the constituents in the hypothesis parse tree look like the constituents in a hand-labeled, gold-reference parse. PARSEVAL thus assumes we have a human-labeled "gold standard" parse tree for each sentence in the test set; we generally draw these gold-standard parses from a treebank like the Penn Treebank. Given these gold-standard reference parses for a test set, a given constituent in a hypothesis parse Ch of a sentence s is labeled "correct" if there is a constituent in the reference parse Cr with the same starting point, ending point, and non-terminal symbol. We can then measure the precision and recall just as we did for chunking in the previous chapter. # of correct constituents in hypothesis parse of s labeled recall: = # of correct constituents in reference parse of s # of correct constituents in hypothesis parse of s labeled precision: = # of total constituents in hypothesis parse of s Subtopic:Summary. This chapter has sketched the basics of probabilistic parsing, concentrating on probabilistic context-free grammars and probabilistic lexicalized context-free grammars. � Probabilistic grammars assign a probability to a sentence or string of words while attempting to capture more sophisticated syntactic information than the N -gram grammars of Chapter 3. � A probabilistic context-free grammar (PCFG) is a context-free grammar in which every rule is annotated with the probability of that rule being chosen. Each PCFG rule is treated as if it were conditionally independent; thus, the probability of a sentence is computed by multiplying the probabilities of each rule in the parse of the sentence. � The probabilistic CKY (Cocke-Kasami-Younger) algorithm is a probabilistic version of the CKY parsing algorithm. There are also probabilistic versions of other parsers like the Earley algorithm. � PCFG probabilities can be learned by counting in a parsed corpus or by parsing a corpus. The inside-outside algorithm is a way of dealing with the fact that the sentences being parsed are ambiguous. � Raw PCFGs suffer from poor independence assumptions among rules and lack of sensitivity to lexical dependencies. � One way to deal with this problem is to split and merge non-terminals (automatically or by hand). 
